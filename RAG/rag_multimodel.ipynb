{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opentelemetry-proto 1.27.0 requires protobuf<5.0,>=3.19, but you have protobuf 5.28.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-status 1.67.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install fitz frontend --quiet\n",
    "!pip install --upgrade pymupdf --quiet\n",
    "!pip install google-generativeai --quiet\n",
    "!pip install langchain-core langchain langchain-community langchain-cohere --quiet\n",
    "!pip install chromadb --quiet\n",
    "!pip install tiktoken --quiet\n",
    "!pip install pillow --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marciacunha/Área de Trabalho/GUPY_DATA_SCIENCE/_sagemaker_ranking_candidates_formation/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import google.generativeai as genai\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_cohere import ChatCohere, CohereEmbeddings\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the \"Attention is all you need\" paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-10-22 12:49:27--  https://arxiv.org/pdf/1706.03762\n",
      "Resolvendo arxiv.org (arxiv.org)... 151.101.131.42, 151.101.3.42, 151.101.195.42, ...\n",
      "Conectando-se a arxiv.org (arxiv.org)|151.101.131.42|:443... conectado.\n",
      "A requisição HTTP foi enviada, aguardando resposta... 200 OK\n",
      "Tamanho: 2215244 (2,1M) [application/pdf]\n",
      "Salvando em: ‘1706.03762’\n",
      "\n",
      "1706.03762          100%[===================>]   2,11M  --.-KB/s    em 0,07s   \n",
      "\n",
      "2024-10-22 12:49:28 (28,8 MB/s) - ‘1706.03762’ salvo [2215244/2215244]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://arxiv.org/pdf/1706.03762\n",
    "!mv 1706.03762 attention_is_all_you_need.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "from PIL import Image\n",
    "import io\n",
    "import glob\n",
    "\n",
    "text_data = []\n",
    "img_data = []\n",
    "    \n",
    "\n",
    "class PDFExtractor:\n",
    "    \n",
    "    def __init__(self, pdf_paths):\n",
    "        \"\"\"\n",
    "        Inicializa a classe com uma lista de caminhos de arquivos PDF.\n",
    "        :param pdf_paths: Lista de caminhos de arquivos PDF\n",
    "        \"\"\"\n",
    "        self.pdf_paths = pdf_paths\n",
    "        self.text_data = []\n",
    "\n",
    "    def extract_from_pdfs(self):\n",
    "        \"\"\"\n",
    "        Extrai texto e imagens de todos os PDFs da lista pdf_paths.\n",
    "        \"\"\"\n",
    "        for pdf_path in self.pdf_paths:\n",
    "            self.extract_from_single_pdf(pdf_path)\n",
    "\n",
    "    def extract_from_single_pdf(self, pdf_path):\n",
    "        \"\"\"\n",
    "        Extrai texto e imagens de um único PDF.\n",
    "        :param pdf_path: Caminho do arquivo PDF\n",
    "        \"\"\"\n",
    "        with fitz.open(pdf_path) as pdf_file:\n",
    "            # Criar um diretório para armazenar as imagens, se não existir\n",
    "            if not os.path.exists(\"extracted_images\"):\n",
    "                os.makedirs(\"extracted_images\")\n",
    "\n",
    "            # Iterar sobre cada página do PDF\n",
    "            for page_number in range(len(pdf_file)):\n",
    "                page = pdf_file[page_number]\n",
    "\n",
    "                # Obter o texto da página\n",
    "                text = page.get_text().strip()\n",
    "                self.text_data.append({\"response\": text, \"name\": f\"{os.path.basename(pdf_path)}_page_{page_number + 1}\"})\n",
    "\n",
    "                # Obter a lista de imagens na página\n",
    "                images = page.get_images(full=True)\n",
    "\n",
    "                # Iterar sobre todas as imagens encontradas na página\n",
    "                for image_index, img in enumerate(images, start=0):\n",
    "                    xref = img[0]  # Obter o XREF da imagem\n",
    "                    base_image = pdf_file.extract_image(xref)  # Extrair a imagem\n",
    "                    image_bytes = base_image[\"image\"]  # Obter os bytes da imagem\n",
    "                    image_ext = base_image[\"ext\"]  # Obter a extensão da imagem\n",
    "\n",
    "                    # Carregar a imagem usando PIL e salvá-la\n",
    "                    image = Image.open(io.BytesIO(image_bytes))\n",
    "                    image.save(f\"extracted_images/{os.path.basename(pdf_path)}_page_{page_number + 1}_{image_index + 1}.{image_ext}\")\n",
    "\n",
    "    def get_text_data(self):\n",
    "        \"\"\"\n",
    "        Retorna o texto extraído de todos os PDFs processados.\n",
    "        \"\"\"\n",
    "        return self.text_data\n",
    "\n",
    "# Caminho do diretório que contém os arquivos PDF\n",
    "directory_path = 'PATH'\n",
    "\n",
    "# Usando glob para listar todos os arquivos PDF no diretório\n",
    "pdf_files = glob.glob(os.path.join(directory_path, \"*.pdf\"))\n",
    "\n",
    "# Exemplo de uso:\n",
    "extractor = PDFExtractor(pdf_files)\n",
    "extractor.extract_from_pdfs()\n",
    "text_data = extractor.get_text_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo gemini-1.5-flash configurado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/google-gemini/cookbook?tab=readme-ov-file\n",
    "#https://aistudio.google.com/app/apikey\n",
    "\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "class GenAIConfigurator:\n",
    "    def __init__(self, api_key=None, model_name=\"gemini-1.5-flash\"):\n",
    "        \"\"\"\n",
    "        Inicializa a classe de configuração para genai com a chave da API e o modelo.\n",
    "\n",
    "        :param api_key: Chave da API genai. Se não for fornecida, tentará buscar da variável de ambiente GENAI_API_KEY.\n",
    "        :param model_name: Nome do modelo a ser usado. O padrão é \"gemini-1.5-flash\".\n",
    "        \"\"\"\n",
    "        self.api_key = api_key or os.getenv('GENAI_API_KEY')\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"A chave da API deve ser fornecida ou estar definida na variável de ambiente 'GENAI_API_KEY'.\")\n",
    "\n",
    "    def configure(self):\n",
    "        \"\"\"\n",
    "        Configura o cliente genai com a chave da API e o modelo especificado.\n",
    "        \"\"\"\n",
    "        genai.configure(api_key=self.api_key)\n",
    "        self.model = genai.GenerativeModel(model_name=self.model_name)\n",
    "        print(f\"Modelo {self.model_name} configurado com sucesso.\")\n",
    "\n",
    "    def get_model(self):\n",
    "        \"\"\"\n",
    "        Retorna o modelo configurado.\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise RuntimeError(\"O modelo não foi configurado. Chame o método configure() primeiro.\")\n",
    "        return self.model\n",
    "\n",
    "# Exemplo de uso:\n",
    "configurator = GenAIConfigurator(api_key='YOUR_KEY')\n",
    "configurator.configure()\n",
    "model = configurator.get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Captioning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "class ImageSummarizer:\n",
    "    def __init__(self, image_dir, model):\n",
    "        \"\"\"\n",
    "        Inicializa a classe para processar as imagens e gerar resumos.\n",
    "\n",
    "        :param image_dir: Diretório onde as imagens estão armazenadas.\n",
    "        :param model: Modelo generativo configurado para gerar os resumos.\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.model = model\n",
    "        self.img_data = []\n",
    "\n",
    "        if not os.path.exists(image_dir):\n",
    "            raise ValueError(f\"O diretório {image_dir} não existe.\")\n",
    "\n",
    "    def summarize_images(self):\n",
    "        \"\"\"\n",
    "        Itera sobre as imagens no diretório e gera resumos usando o modelo.\n",
    "        \"\"\"\n",
    "        for img_file in os.listdir(self.image_dir):\n",
    "            img_path = os.path.join(self.image_dir, img_file)\n",
    "            image = Image.open(img_path)\n",
    "            response = self.model.generate_content([\n",
    "                image,\n",
    "                \"You are an assistant tasked with summarizing tables, images and text for retrieval. These summaries will be embedded and used to retrieve the raw text or table elements. Give a concise summary of the table or text that is well optimized for retrieval. Table or text or image:\"\n",
    "            ])\n",
    "            self.img_data.append({\"response\": response.text, \"name\": img_file})\n",
    "\n",
    "    def get_summaries(self):\n",
    "        \"\"\"\n",
    "        Retorna os resumos gerados para todas as imagens.\n",
    "\n",
    "        :return: Lista de dicionários contendo os resumos e os nomes das imagens.\n",
    "        \"\"\"\n",
    "        return self.img_data\n",
    "\n",
    "# Exemplo de uso:\n",
    "# model = <instância do modelo configurado anteriormente>\n",
    "image_summarizer = ImageSummarizer(image_dir=\"extracted_images\", model=model)\n",
    "image_summarizer.summarize_images()\n",
    "summaries = image_summarizer.get_summaries()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'response': 'This image shows a scaled dot-product attention mechanism, which is used to compute the attention weights that are then applied to a value matrix. The mechanism first calculates the dot product of a query matrix and a key matrix. The result is then scaled and masked. Finally, a softmax operation is applied to compute the attention weights. The attention weights are then multiplied by the value matrix. The inputs to the scaled dot-product attention are Q, K, and V, which stand for query, key, and value.',\n",
       "  'name': 'image_4_1.png'},\n",
       " {'response': 'This image illustrates the architecture of a transformer model, commonly used in natural language processing.  The input to the model is a sequence of words, which are first embedded into a vector space and then augmented with positional encodings. These encodings are processed in a series of encoder layers, each of which consists of two sub-layers: a multi-head attention layer and a feed-forward neural network. The encoder layers encode the input sequence into a representation that captures its meaning and context. The output from the encoders is then passed to a series of decoder layers. The decoder layers generate the output sequence, one word at a time, using a similar architecture to the encoder layers. The decoder layers also use a multi-head attention mechanism to attend to the encoded input sequence, ensuring that the generated output is consistent with the context of the input. Finally, the output sequence is passed through a linear layer and a softmax function to produce a probability distribution over the vocabulary, which is then used to predict the next word in the sequence.',\n",
       "  'name': 'image_3_1.png'},\n",
       " {'response': 'This diagram shows the architecture of the Scaled Dot-Product Attention. It takes 3 inputs: V, K, and Q. These inputs are transformed by a linear layer and then concatenated together. This concatenation is then passed through the Scaled Dot-Product Attention layer, which outputs a vector that is then transformed by another linear layer.',\n",
       "  'name': 'attention_is_all_you_need.pdf_page_4_2.png'},\n",
       " {'response': 'This image shows a diagram of a scaled dot-product attention mechanism. This mechanism is used in neural networks for natural language processing, and is designed to focus on the most relevant parts of the input sequence when calculating the output. It works by first projecting the input sequences into three representations: query (Q), key (K), and value (V). The query is then used to calculate the attention weights for each key. These weights are then used to combine the values to produce the output. The image shows the linear layers that are used to project the input sequences into the query, key, and value representations. It also shows how the attention weights are calculated and used to combine the values.',\n",
       "  'name': 'image_4_2.png'},\n",
       " {'response': 'This image depicts the architecture of a transformer model, a type of neural network often used for natural language processing. It shows the encoder and decoder components of the model, as well as the different layers that are used in each component. The encoder takes an input sequence of tokens (words, subwords, etc.) and encodes it into a representation that captures the meaning of the sequence. The decoder then uses this representation to generate an output sequence, such as a translation, summary, or question answer. The key components of the encoder and decoder are: \\n\\n* **Input Embedding:** Converts input tokens into vector representations. \\n* **Positional Encoding:** Adds information about the position of each token in the sequence. \\n* **Multi-Head Attention:** Captures relationships between different parts of the input sequence. \\n* **Feedforward Neural Network:** Processes the output of the attention layer to further refine the representation. \\n* **Add & Norm:** Adds the output of the feedforward neural network to the input of the layer and normalizes the result. \\n\\nThe decoder also uses a multi-head attention layer to capture relationships between different parts of the output sequence. This is called \"masked\" multi-head attention because it prevents the decoder from seeing tokens that have not yet been generated. \\n\\nThe image also shows the output probabilities, which are used to determine the most likely output token for each position in the output sequence. \\n',\n",
       "  'name': 'attention_is_all_you_need.pdf_page_3_1.png'},\n",
       " {'response': 'This image depicts the scaled dot-product attention mechanism used in transformers. The image shows the steps involved in calculating attention, including the matrix multiplication of the query (Q) and key (K) vectors, the application of a mask (optional), the softmax normalization, and the final matrix multiplication with the value (V) vectors. The process is represented as a flow diagram, showing the input and output of each step.',\n",
       "  'name': 'attention_is_all_you_need.pdf_page_4_1.png'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectostore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de embeddings embed-english-v3.0 configurado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# Set embeddings\n",
    "#https://python.langchain.com/docs/integrations/text_embedding/cohere/\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "from langchain_cohere import ChatCohere, CohereEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "class DocumentImageProcessor:\n",
    "    def __init__(self, api_key=None, embedding_model_name=\"embed-english-v3.0\", collection_name=\"multi_model_rag\"):\n",
    "        \"\"\"\n",
    "        Inicializa a classe para configurar a chave da API Cohere e processar documentos e imagens.\n",
    "\n",
    "        :param api_key: Chave da API Cohere. Se não for fornecida, tenta buscar da variável de ambiente COHERE_API_KEY.\n",
    "        :param embedding_model_name: Nome do modelo de embeddings da Cohere. Padrão: \"embed-english-v3.0\".\n",
    "        \"\"\"\n",
    "        self.api_key = api_key or os.getenv(\"COHERE_API_KEY\")\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.embedding_model = None\n",
    "        self.collection_name = collection_name\n",
    "        self.vectorstore = None\n",
    "\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"A chave da API deve ser fornecida ou definida na variável de ambiente 'COHERE_API_KEY'.\")\n",
    "        self._set_api_key()\n",
    "\n",
    "    def _set_api_key(self):\n",
    "        \"\"\"\n",
    "        Define a chave da API no ambiente se não estiver presente.\n",
    "        \"\"\"\n",
    "        if not os.getenv(\"COHERE_API_KEY\"):\n",
    "            os.environ[\"COHERE_API_KEY\"] = self.api_key\n",
    "\n",
    "    def configure_model(self):\n",
    "        \"\"\"\n",
    "        Configura o modelo de embeddings da Cohere.\n",
    "        \"\"\"\n",
    "        self.embedding_model = CohereEmbeddings(model=self.embedding_model_name)\n",
    "        print(f\"Modelo de embeddings {self.embedding_model_name} configurado com sucesso.\")\n",
    "\n",
    "    def process_documents_and_images(self, text_data, img_data, chunk_size=400, chunk_overlap=50):\n",
    "        \"\"\"\n",
    "        Carrega e divide documentos e imagens em partes menores.\n",
    "\n",
    "        :param text_data: Lista de textos para criar os documentos.\n",
    "        :param img_data: Lista de resumos de imagens para criar os documentos.\n",
    "        :param chunk_size: Tamanho dos chunks após a divisão.\n",
    "        :param chunk_overlap: Quantidade de sobreposição entre os chunks.\n",
    "        :return: Documentos e imagens divididos em partes menores.\n",
    "        \"\"\"\n",
    "        # Carregar documentos e imagens\n",
    "        docs_list = [Document(page_content=text['response'], metadata={\"name\": text['name']}) for text in text_data]\n",
    "        img_list = [Document(page_content=img['response'], metadata={\"name\": img['name']}) for img in img_data]\n",
    "\n",
    "        # Dividir em chunks menores\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "        )\n",
    "\n",
    "        doc_splits = text_splitter.split_documents(docs_list)\n",
    "        img_splits = text_splitter.split_documents(img_list)\n",
    "\n",
    "        return doc_splits, img_splits\n",
    "    \n",
    "    def add_to_vectorstore(self, doc_splits, img_splits):\n",
    "        \"\"\"\n",
    "        Adiciona os documentos ao vectorstore usando Chroma.\n",
    "\n",
    "        :param doc_splits: Divisões de documentos de texto.\n",
    "        :param img_splits: Divisões de documentos de imagem.\n",
    "        \"\"\"\n",
    "        # Adicionando ao vectorstore\n",
    "        self.vectorstore = Chroma.from_documents(\n",
    "            documents=doc_splits + img_splits,  # texto e imagens\n",
    "            collection_name=self.collection_name,\n",
    "            embedding=self.embedding_model\n",
    "        )\n",
    "\n",
    "    def get_retriever(self, k=1):\n",
    "        \"\"\"\n",
    "        Cria um retriever para buscar documentos similares.\n",
    "\n",
    "        :param k: Número de documentos a recuperar (padrão 1).\n",
    "        :return: Um retriever configurado.\n",
    "        \"\"\"\n",
    "        if self.vectorstore is None:\n",
    "            raise ValueError(\"Vectorstore não foi inicializado. Adicione os documentos primeiro.\")\n",
    "\n",
    "        # Criando o retriever com base na similaridade\n",
    "        retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={'k': k}\n",
    "        )\n",
    "        return retriever\n",
    "    \n",
    "\n",
    "# Exemplo de uso:\n",
    "# text_data e img_data devem ser listas de dicionários com o conteúdo de resposta e nome.\n",
    "processor = DocumentImageProcessor(api_key='YOUR_KEY')\n",
    "processor.configure_model()\n",
    "doc_splits, img_splits = processor.process_documents_and_images(text_data, img_data)\n",
    "# Adicionando ao vectorstore\n",
    "processor.add_to_vectorstore(doc_splits, img_splits)\n",
    "\n",
    "# Criando retriever\n",
    "retriever = processor.get_retriever(k=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'CohereEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7587a155ee20>, search_kwargs={'k': 1})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query |Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer (base model) achieves a BLEU score of 27.3.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_cohere import ChatCohere, CohereEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class RAGQuery:\n",
    "    def __init__(self, retriever, llm_model=\"command-r-plus\", temperature=0, system_message=None):\n",
    "        \"\"\"\n",
    "        Inicializa a classe com o retriever e o LLM.\n",
    "\n",
    "        :param retriever: O retriever configurado para buscar documentos.\n",
    "        :param llm_model: Nome do modelo LLM a ser usado (padrão: \"command-r-plus\").\n",
    "        :param temperature: Valor de temperatura para controle de criatividade do LLM (padrão 0).\n",
    "        :param system_message: Mensagem padrão do sistema a ser usada no prompt (padrão é um sistema para tarefas de QA).\n",
    "        \"\"\"\n",
    "        self.retriever = retriever\n",
    "        self.llm = ChatCohere(model=llm_model, temperature=temperature)\n",
    "\n",
    "        # Definindo a mensagem do sistema\n",
    "        self.system_message = system_message or \"\"\"You are an assistant for question-answering tasks. Answer the question based upon your knowledge. \n",
    "        Use three-to-five sentences maximum and keep the answer concise.\"\"\"\n",
    "\n",
    "    def run_query(self, query, max_docs=1):\n",
    "        \"\"\"\n",
    "        Realiza uma consulta ao retriever, cria o prompt e executa a geração da resposta.\n",
    "\n",
    "        :param query: A pergunta feita pelo usuário.\n",
    "        :param max_docs: Número máximo de documentos a recuperar (padrão 1).\n",
    "        :return: A resposta gerada pelo LLM.\n",
    "        \"\"\"\n",
    "        # Recuperando os documentos\n",
    "        docs = self.retriever.invoke(query)[:max_docs]\n",
    "\n",
    "        # Definindo o template de prompt\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", self.system_message),\n",
    "                (\"human\", \"Retrieved documents: \\n\\n <docs>{documents}</docs> \\n\\n User question: <question>{question}</question>\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Criando o encadeamento RAG\n",
    "        rag_chain = prompt | self.llm | StrOutputParser()\n",
    "\n",
    "        # Executando a geração\n",
    "        generation = rag_chain.invoke({\"documents\": docs[0].page_content, \"question\": query})\n",
    "        return generation\n",
    "\n",
    "# Exemplo de uso:\n",
    "if __name__ == \"__main__\":\n",
    "    # Suponha que 'retriever' já esteja configurado\n",
    "    query = \"What is the BLEU score of the Transformer (base model)?\"\n",
    "    \n",
    "    # Criando instância da classe\n",
    "    rag_query_class = RAGQuery(retriever)\n",
    "\n",
    "    # Executando a consulta\n",
    "    answer = rag_query_class.run_query(query)\n",
    "    print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
