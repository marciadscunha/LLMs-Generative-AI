{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers pandas --quiet\n",
    "!pip install -U scikit-learn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marciacunha/.pyenv/versions/3.9.6/envs/rank_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Configuração do modelo e tokenizer\n",
    "model_name = \"neuralmind/bert-base-portuguese-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Função para calcular importância das palavras-chave (tokens) com base nos pesos de atenção\n",
    "def calculate_keyword_importance(attentions, tokens):\n",
    "    token_importance = attentions[-1].mean(dim=1).squeeze().tolist()\n",
    "    keywords = [(tokenizer.decode([token]), importance) for token, importance in zip(tokens, token_importance)]\n",
    "    keywords_sorted = sorted(keywords, key=lambda x: x[1], reverse=True)\n",
    "    return keywords_sorted[:5]  # Top 5 palavras-chave mais importantes\n",
    "\n",
    "# Função principal para gerar o dataframe de explicabilidade\n",
    "def generate_explainability_df(text_1, text_2):\n",
    "    # Tokenizar as entradas e gerar embeddings com atenção\n",
    "    inputs_1 = tokenizer(text_1, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs_2 = tokenizer(text_2, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Embeddings e pesos de atenção\n",
    "    outputs_1 = model(**inputs_1, output_attentions=True)\n",
    "    outputs_2 = model(**inputs_2, output_attentions=True)\n",
    "\n",
    "    # Similaridade de cosseno entre os embeddings finais\n",
    "    embeddings_1 = outputs_1.last_hidden_state[:, 0, :].detach().numpy()\n",
    "    embeddings_2 = outputs_2.last_hidden_state[:, 0, :].detach().numpy()\n",
    "    similarity = cosine_similarity(embeddings_1, embeddings_2)[0][0]\n",
    "\n",
    "    # Extração e cálculo de pesos de atenção\n",
    "    attention_weights_1 = [layer.detach().numpy() for layer in outputs_1.attentions]\n",
    "    attention_weights_2 = [layer.detach().numpy() for layer in outputs_2.attentions]\n",
    "\n",
    "    # Palavras-chave mais importantes\n",
    "    keywords_1 = calculate_keyword_importance(outputs_1.attentions, inputs_1[\"input_ids\"].squeeze())\n",
    "    keywords_2 = calculate_keyword_importance(outputs_2.attentions, inputs_2[\"input_ids\"].squeeze())\n",
    "\n",
    "    # Confiança na Similaridade\n",
    "    confidence_score = np.std(np.mean(attention_weights_1[-1], axis=1).squeeze().tolist())\n",
    "    \n",
    "    # Justificativa\n",
    "    justification_summary = \"Similaridade baseada em termos como \" + \", \".join([kw[0] for kw in keywords_1[:3]]) \\\n",
    "                            + \" no texto 1 e \" + \", \".join([kw[0] for kw in keywords_2[:3]]) + \" no texto 2.\"\n",
    "\n",
    "    # Informação de Tamanho\n",
    "    input_length_1 = len(inputs_1[\"input_ids\"].squeeze())\n",
    "    input_length_2 = len(inputs_2[\"input_ids\"].squeeze())\n",
    "\n",
    "    # Tokens ignorados\n",
    "    ignored_tokens_1 = [tokenizer.decode([token]) for token in inputs_1[\"input_ids\"].squeeze().tolist() if token not in tokenizer.all_special_ids]\n",
    "    ignored_tokens_2 = [tokenizer.decode([token]) for token in inputs_2[\"input_ids\"].squeeze().tolist() if token not in tokenizer.all_special_ids]\n",
    "\n",
    "    # Clusterização e Distância para Outliers (Exemplo com uma métrica simples)\n",
    "    embedding_cluster_positions_1 = embeddings_1.tolist()\n",
    "    embedding_cluster_positions_2 = embeddings_2.tolist()\n",
    "    outlier_distance = np.linalg.norm(embeddings_1 - embeddings_2)  # Distância entre os embeddings\n",
    "\n",
    "    # Criar o DataFrame com as informações explicáveis\n",
    "    df = pd.DataFrame({\n",
    "        \"input_text_1\": [text_1],\n",
    "        \"input_text_2\": [text_2],\n",
    "        \"token_ids_1\": [inputs_1[\"input_ids\"].detach().numpy()],\n",
    "        \"token_ids_2\": [inputs_2[\"input_ids\"].detach().numpy()],\n",
    "        \"attention_weights_1\": [attention_weights_1],\n",
    "        \"attention_weights_2\": [attention_weights_2],\n",
    "        \"similarity_score\": [similarity],\n",
    "        \"embedding_vectors_1\": [embeddings_1],\n",
    "        \"embedding_vectors_2\": [embeddings_2],\n",
    "        \"timestamp\": [datetime.now()],\n",
    "        \"model_version\": [model_name],\n",
    "        \"keyword_importance_1\": [keywords_1],\n",
    "        \"keyword_importance_2\": [keywords_2],\n",
    "        \"similarity_confidence\": [confidence_score],\n",
    "        \"justification_summary\": [justification_summary],\n",
    "        \"input_length_1\": [input_length_1],\n",
    "        \"input_length_2\": [input_length_2],\n",
    "        \"ignored_tokens_1\": [ignored_tokens_1],\n",
    "        \"ignored_tokens_2\": [ignored_tokens_2],\n",
    "        \"embedding_cluster_positions_1\": [embedding_cluster_positions_1],\n",
    "        \"embedding_cluster_positions_2\": [embedding_cluster_positions_2],\n",
    "        \"outlier_distance\": [outlier_distance]\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Exemplo de uso\n",
    "text_1 = \"Descrição de uma vaga de cientista de dados\"\n",
    "text_2 = \"Perfil de um candidato com experiência em análise de dados\"\n",
    "df_explainability = generate_explainability_df(text_1, text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text_1</th>\n",
       "      <th>input_text_2</th>\n",
       "      <th>token_ids_1</th>\n",
       "      <th>token_ids_2</th>\n",
       "      <th>attention_weights_1</th>\n",
       "      <th>attention_weights_2</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>embedding_vectors_1</th>\n",
       "      <th>embedding_vectors_2</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>...</th>\n",
       "      <th>keyword_importance_2</th>\n",
       "      <th>similarity_confidence</th>\n",
       "      <th>justification_summary</th>\n",
       "      <th>input_length_1</th>\n",
       "      <th>input_length_2</th>\n",
       "      <th>ignored_tokens_1</th>\n",
       "      <th>ignored_tokens_2</th>\n",
       "      <th>embedding_cluster_positions_1</th>\n",
       "      <th>embedding_cluster_positions_2</th>\n",
       "      <th>outlier_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Descrição de uma vaga de cientista de dados</td>\n",
       "      <td>Perfil de um candidato com experiência em anál...</td>\n",
       "      <td>[[101, 1305, 10950, 125, 230, 5926, 125, 10989...</td>\n",
       "      <td>[[101, 1740, 3252, 125, 222, 4931, 170, 4040, ...</td>\n",
       "      <td>[[[[[0.8595248  0.01187342 0.00428111 0.011538...</td>\n",
       "      <td>[[[[[0.83092827 0.0191794  0.01446818 0.011154...</td>\n",
       "      <td>0.771395</td>\n",
       "      <td>[[0.10359708, -0.48410273, 1.0135684, -0.19191...</td>\n",
       "      <td>[[0.116354845, -0.41202834, 0.5305816, -0.0884...</td>\n",
       "      <td>2024-11-12 11:40:30.366627</td>\n",
       "      <td>...</td>\n",
       "      <td>[(um, [0.3526493012905121, 0.03881805762648582...</td>\n",
       "      <td>0.121607</td>\n",
       "      <td>Similaridade baseada em termos como de, uma, d...</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>[Des, ##crição, de, uma, vaga, de, cientista, ...</td>\n",
       "      <td>[Per, ##fil, de, um, candidato, com, experiênc...</td>\n",
       "      <td>[[0.10359708219766617, -0.484102725982666, 1.0...</td>\n",
       "      <td>[[0.11635484546422958, -0.41202834248542786, 0...</td>\n",
       "      <td>7.38531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  input_text_1  \\\n",
       "0  Descrição de uma vaga de cientista de dados   \n",
       "\n",
       "                                        input_text_2  \\\n",
       "0  Perfil de um candidato com experiência em anál...   \n",
       "\n",
       "                                         token_ids_1  \\\n",
       "0  [[101, 1305, 10950, 125, 230, 5926, 125, 10989...   \n",
       "\n",
       "                                         token_ids_2  \\\n",
       "0  [[101, 1740, 3252, 125, 222, 4931, 170, 4040, ...   \n",
       "\n",
       "                                 attention_weights_1  \\\n",
       "0  [[[[[0.8595248  0.01187342 0.00428111 0.011538...   \n",
       "\n",
       "                                 attention_weights_2  similarity_score  \\\n",
       "0  [[[[[0.83092827 0.0191794  0.01446818 0.011154...          0.771395   \n",
       "\n",
       "                                 embedding_vectors_1  \\\n",
       "0  [[0.10359708, -0.48410273, 1.0135684, -0.19191...   \n",
       "\n",
       "                                 embedding_vectors_2  \\\n",
       "0  [[0.116354845, -0.41202834, 0.5305816, -0.0884...   \n",
       "\n",
       "                   timestamp  ...  \\\n",
       "0 2024-11-12 11:40:30.366627  ...   \n",
       "\n",
       "                                keyword_importance_2 similarity_confidence  \\\n",
       "0  [(um, [0.3526493012905121, 0.03881805762648582...              0.121607   \n",
       "\n",
       "                               justification_summary  input_length_1  \\\n",
       "0  Similaridade baseada em termos como de, uma, d...              11   \n",
       "\n",
       "  input_length_2                                   ignored_tokens_1  \\\n",
       "0             13  [Des, ##crição, de, uma, vaga, de, cientista, ...   \n",
       "\n",
       "                                    ignored_tokens_2  \\\n",
       "0  [Per, ##fil, de, um, candidato, com, experiênc...   \n",
       "\n",
       "                       embedding_cluster_positions_1  \\\n",
       "0  [[0.10359708219766617, -0.484102725982666, 1.0...   \n",
       "\n",
       "                       embedding_cluster_positions_2 outlier_distance  \n",
       "0  [[0.11635484546422958, -0.41202834248542786, 0...          7.38531  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_explainability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rank_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
